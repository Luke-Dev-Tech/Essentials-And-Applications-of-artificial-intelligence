{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "54e47d3d665b4e7981e402610469677a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_def502b1f5f14234bb700bbd34e5d626",
              "IPY_MODEL_5b0b8d55b8404e52bf37ea1af53fa01b",
              "IPY_MODEL_44cc07fe89b64a0baa0c4ad3a7d1e9c5"
            ],
            "layout": "IPY_MODEL_f0d0a998336543e2acb3f5461130d6bd"
          }
        },
        "def502b1f5f14234bb700bbd34e5d626": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ab5553f68c504fb882eb009583454fff",
            "placeholder": "​",
            "style": "IPY_MODEL_ee0afa93d8724902b25310aede1106b2",
            "value": "100%"
          }
        },
        "5b0b8d55b8404e52bf37ea1af53fa01b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8628dbff07064f61929cc2ab7b70d42c",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1c41ffea3b704f50b48fc63e8a3ac813",
            "value": 5
          }
        },
        "44cc07fe89b64a0baa0c4ad3a7d1e9c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d0548f8f58c0452fbf453061e865cdad",
            "placeholder": "​",
            "style": "IPY_MODEL_3583f3c3a1b046639741cfc72166e29b",
            "value": " 5/5 [01:16&lt;00:00, 14.98s/it]"
          }
        },
        "f0d0a998336543e2acb3f5461130d6bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab5553f68c504fb882eb009583454fff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee0afa93d8724902b25310aede1106b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8628dbff07064f61929cc2ab7b70d42c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c41ffea3b704f50b48fc63e8a3ac813": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d0548f8f58c0452fbf453061e865cdad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3583f3c3a1b046639741cfc72166e29b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# [1] Kaggle Json"
      ],
      "metadata": {
        "id": "RqIF0f1vgu3C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "kaggle_json = \"/content/drive/My Drive/UWE Projects/EAI/kaggle.json\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qFttMh5guDd",
        "outputId": "9005a918-6308-4120-9f74-672b6a3f6810"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "shutil.copyfile(kaggle_json, os.path.expanduser('~/.kaggle/kaggle.json'))\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "QdfVC1q2hVFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [2] Modular Section"
      ],
      "metadata": {
        "id": "4JP6rB-sfZXK"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01371af1"
      },
      "source": [
        "!mkdir -p modular"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [2.1] kaggle_data.py"
      ],
      "metadata": {
        "id": "O2p1vRvG6HSv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "noqev_zCelvX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fed93a1d-da4e-4a10-fad1-4321b3cd81e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing modular/kaggle_data.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile modular/kaggle_data.py\n",
        "import os\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "import subprocess\n",
        "\n",
        "def kaggleDataImport(dataset_name: str):\n",
        "    \"\"\"\n",
        "    dataset_name example:\n",
        "    'feyzazkefe/trashnet'\n",
        "    \"\"\"\n",
        "\n",
        "    data_path = Path(\"data\")\n",
        "    data_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    zip_name = dataset_name.split(\"/\")[-1] + \".zip\"\n",
        "    zip_path = data_path / zip_name\n",
        "\n",
        "    # ---------------- Download ----------------\n",
        "    if not zip_path.exists():\n",
        "        print(f\"[Info] Downloading {dataset_name} ...\")\n",
        "        subprocess.run(\n",
        "            [\n",
        "                \"kaggle\",\n",
        "                \"datasets\",\n",
        "                \"download\",\n",
        "                \"-d\",\n",
        "                dataset_name,\n",
        "                \"-p\",\n",
        "                str(data_path)\n",
        "            ],\n",
        "            check=True\n",
        "        )\n",
        "    else:\n",
        "        print(\"[Success] Zip already exists\")\n",
        "\n",
        "    # ---------------- Unzip ----------------\n",
        "    print(\"[Info] Extracting dataset...\")\n",
        "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
        "        zip_ref.extractall(data_path)\n",
        "\n",
        "    # ---------------- Detect extracted folder ----------------\n",
        "    extracted_items = [\n",
        "        p for p in data_path.iterdir()\n",
        "        if p.is_dir() and p.name != \"__MACOSX\"\n",
        "    ]\n",
        "\n",
        "    if len(extracted_items) == 1:\n",
        "        dataset_dir = extracted_items[0]\n",
        "    else:\n",
        "        # Multiple folders → choose most recent\n",
        "        dataset_dir = max(extracted_items, key=lambda p: p.stat().st_mtime)\n",
        "\n",
        "    print(\"[Success] Dataset ready at:\", dataset_dir)\n",
        "\n",
        "    # ---------------- Cleanup ----------------\n",
        "    os.remove(zip_path)\n",
        "\n",
        "    return dataset_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [2.2] data_setup.py"
      ],
      "metadata": {
        "id": "WsejebNbqRuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile modular/data_setup.py\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "\n",
        "NUM_WORKERS = os.cpu_count()\n",
        "\n",
        "def create_dataloaders(\n",
        "    data_dir: str,\n",
        "    train_transform: transforms.Compose,\n",
        "    test_transform: transforms.Compose,\n",
        "    batch_size: int,\n",
        "    test_size: float = 0.2,\n",
        "    random_state: int = 42,\n",
        "    num_workers: int = NUM_WORKERS\n",
        "):\n",
        "    train_data = datasets.ImageFolder(root=data_dir, transform=train_transform)\n",
        "    test_data = datasets.ImageFolder(root=data_dir, transform=test_transform)\n",
        "\n",
        "    class_names = train_data.classes\n",
        "    indices = list(range(len(train_data)))\n",
        "\n",
        "    train_idx, test_idx = train_test_split(\n",
        "        indices,\n",
        "        test_size=test_size,\n",
        "        random_state=random_state,\n",
        "        shuffle=True,\n",
        "        stratify=train_data.targets\n",
        "    )\n",
        "\n",
        "    train_dataset = Subset(train_data, train_idx)\n",
        "    test_dataset = Subset(test_data, test_idx)\n",
        "\n",
        "    print(f\"[INFO] Total images: {len(train_data)}\")\n",
        "    print(f\"[INFO] Training images: {len(train_dataset)}\")\n",
        "    print(f\"[INFO] Testing images: {len(test_dataset)}\")\n",
        "\n",
        "    train_dataloader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    test_dataloader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    return (train_dataloader, test_dataloader, class_names, train_dataset, test_dataset, train_idx, test_idx)"
      ],
      "metadata": {
        "id": "eqerwIe5kOSd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98426fb4-6c36-49c4-8f44-5fd9a025d8b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing modular/data_setup.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [2.3] engine.py"
      ],
      "metadata": {
        "id": "I9Lm7ygWkZFz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile modular/engine.py\n",
        "\n",
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "# Main Method (1): Train Step\n",
        "def train_step(model: torch.nn.Module,\n",
        "               dataloader: torch.utils.data.DataLoader,\n",
        "               loss_fun: torch.nn.Module,\n",
        "               optimizer: torch.optim.Optimizer,\n",
        "               device: torch.device):\n",
        "  model.train()\n",
        "  train_loss, train_acc = 0, 0\n",
        "\n",
        "  for batch, (X,y) in enumerate(dataloader):\n",
        "    X, y = X.to(device), y.to(device)\n",
        "    # Do the forward pass\n",
        "    y_pred = model(X)\n",
        "\n",
        "    # Calculate and Accumulate the loss\n",
        "    loss = loss_fun(y_pred, y)\n",
        "    train_loss += loss.item()\n",
        "\n",
        "    # Optimizer Zero\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Loss backward\n",
        "    loss.backward()\n",
        "\n",
        "    # opitmizer step\n",
        "    optimizer.step()\n",
        "\n",
        "    # Calculate and accumulate accuracy metric acorss all batches\n",
        "    y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
        "    train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n",
        "\n",
        "  train_loss = train_loss / len(dataloader)\n",
        "  train_acc = train_acc / len(dataloader)\n",
        "  return train_loss, train_acc\n",
        "\n",
        "# Main Method (2): Test Step\n",
        "def test_step(model: torch.nn.Module,\n",
        "              dataloader: torch.utils.data.DataLoader,\n",
        "              loss_fun: torch.nn.Module,\n",
        "              optimizer: torch.optim.Optimizer,\n",
        "              device: torch.device\n",
        "              ):\n",
        "  model.eval()\n",
        "  test_loss, test_acc = 0, 0\n",
        "  with torch.inference_mode():\n",
        "    for batch, (X,y) in enumerate(dataloader):\n",
        "      X, y = X.to(device), y.to(device)\n",
        "      y_pred = model(X)\n",
        "      loss = loss_fun(y_pred, y)\n",
        "      test_loss += loss.item()\n",
        "      test_acc += (torch.argmax(torch.softmax(y_pred, dim=1), dim=1) == y).sum().item()/len(y_pred)\n",
        "\n",
        "  test_loss = test_loss / len(dataloader)\n",
        "  test_acc = test_acc / len(dataloader)\n",
        "  return test_loss, test_acc\n",
        "\n",
        "# Main Method (3): Initiate\n",
        "def initiate_train_test(model: torch.nn.Module,\n",
        "           train_dataloader: torch.utils.data.DataLoader,\n",
        "           test_dataloader: torch.utils.data.DataLoader,\n",
        "           optimizer: torch.optim.Optimizer,\n",
        "           loss_fun: torch.nn.Module,\n",
        "           epochs: int,\n",
        "           device: torch.device) -> Dict[str, List]:\n",
        "  \"\"\"\n",
        "  :\n",
        "  :return: Retrun a dictionary containing Training loss, training acc, test loss and test acc.\n",
        "  This initiate training and testing processes by using the data loaded by dataloader.\n",
        "\n",
        "  \"\"\"\n",
        "  results = {\"train_loss\": [],\n",
        "             \"train_acc\": [],\n",
        "             \"test_loss\": [],\n",
        "             \"test_acc\": []\n",
        "             }\n",
        "\n",
        "  for epoch in tqdm(range(epochs)):\n",
        "    train_loss, train_acc = train_step(model=model,\n",
        "                                       dataloader=train_dataloader,\n",
        "                                       loss_fun=loss_fun,\n",
        "                                       optimizer=optimizer,\n",
        "                                       device=device)\n",
        "    test_loss, test_acc = test_step(model=model,\n",
        "                                    dataloader=test_dataloader,\n",
        "                                    loss_fun=loss_fun,\n",
        "                                    optimizer=optimizer,\n",
        "                                    device=device)\n",
        "\n",
        "    print(\n",
        "        f\"Epoch: {epoch+1} | \"\n",
        "        f\"train_loss: {train_loss:.4f} | \"\n",
        "        f\"train_acc: {train_acc:.4f} | \"\n",
        "        f\"test_loss: {test_loss:.4f} | \"\n",
        "        f\"test_acc: {test_acc:.4f}\"\n",
        "    )\n",
        "\n",
        "    results[\"train_loss\"].append(train_loss)\n",
        "    results[\"train_acc\"].append(train_acc)\n",
        "    results[\"test_loss\"].append(test_loss)\n",
        "    results[\"test_acc\"].append(test_acc)\n",
        "\n",
        "  return results\n",
        "\n"
      ],
      "metadata": {
        "id": "meckiLcJkXk3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0690d5dd-ea92-4dc2-8d7b-08f9d9800c82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing modular/engine.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [2.4] utlis.py"
      ],
      "metadata": {
        "id": "kd9lDKn8kbw7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile modular/utils.py\n",
        "\n",
        "import torch\n",
        "from pathlib import Path\n",
        "\n",
        "def save_model(model: torch.nn.Module,\n",
        "               target_dir: str,\n",
        "               model_name:str):\n",
        "  target_dir_path = Path(target_dir)\n",
        "  target_dir_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "  model_save_path = target_dir_path / model_name\n",
        "\n",
        "  print(f\"[INFO] Saving model to: {model_save_path}\")\n",
        "  torch.save(obj=model.state_dict(), f=model_save_path)\n"
      ],
      "metadata": {
        "id": "VlXqaJUBkd3Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1820f62e-5e9c-4900-ffbb-06cf42478f52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing modular/utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [2.5] pred_and_plot_img.py"
      ],
      "metadata": {
        "id": "Ri3GNG2Gkiuk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile modular/pred_and_plot_img.py\n",
        "from typing import List, Tuple\n",
        "import torch\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "\n",
        "def pred_and_plot_img(\n",
        "    model: torch.nn.Module,\n",
        "    image_path: str,\n",
        "    class_name: List[str],\n",
        "    image_size: Tuple[int, int] = (224, 224),\n",
        "    transform: torchvision.transforms = None,\n",
        "    device: torch.device = device\n",
        "):\n",
        "    # Load image\n",
        "    img_pil = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "    # Use provided transform or default VGG-Face transform\n",
        "    if transform is None:\n",
        "        transform = torchvision.transforms.Compose([\n",
        "            torchvision.transforms.Resize(image_size),\n",
        "            torchvision.transforms.Grayscale(num_output_channels=3),\n",
        "            torchvision.transforms.ToTensor(),\n",
        "            transforms.Normalize(\n",
        "              mean=[0.485, 0.456, 0.406],\n",
        "              std=[0.229, 0.224, 0.225]\n",
        "          )\n",
        "        ])\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        img_tensor = transform(img_pil).unsqueeze(0).to(device)\n",
        "        logits = model(img_tensor)\n",
        "        probs = torch.softmax(logits, dim=1)\n",
        "        pred_idx = probs.argmax(dim=1).item()\n",
        "        pred_prob = probs.max().item()\n",
        "\n",
        "    actual_label = Path(image_path).parent.name\n",
        "    pred_label = class_name[pred_idx]\n",
        "\n",
        "    plt.figure(figsize=(5, 5))\n",
        "    plt.imshow(img_pil)\n",
        "    plt.title(\n",
        "        f\"Actual: {actual_label} | Pred: {pred_label} | Prob: {pred_prob:.3f}\"\n",
        "    )\n",
        "    plt.axis(\"off\")\n"
      ],
      "metadata": {
        "id": "WXs1nqBYkkj8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dee599bb-dda7-4a19-a265-392bf41d5ee9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing modular/pred_and_plot_img.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [3] Pipeline"
      ],
      "metadata": {
        "id": "Iox7Um1-kpQk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision\n",
        "from modular import kaggle_data, data_setup, engine, utils, pred_and_plot_img\n",
        "from timeit import default_timer as timer\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  #====================Device Setup=============================\n",
        "  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "  #=============================================================\n",
        "  # Kaggle Data Import\n",
        "  kaggle_data.kaggleDataImport('feyzazkefe/trashnet')\n",
        "  #=================Attribute Section===========================\n",
        "  # NUM_EPOCHS = 30\n",
        "  BATCH_SIZE = 32\n",
        "  # HIDDEN_UNITS = 10\n",
        "  # LEARNING_RATE = 0.0005\n",
        "  data_dir = 'data/dataset-resized'\n",
        "  #=============================================================\n",
        "  # Data_setup\n",
        "  train_transform = transforms.Compose([\n",
        "      transforms.Resize((260, 260)),\n",
        "      transforms.RandomHorizontalFlip(p=0.5),\n",
        "      transforms.RandomRotation(10),\n",
        "      transforms.Grayscale(num_output_channels=3),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize(\n",
        "          mean=[0.485, 0.456, 0.406],\n",
        "          std=[0.229, 0.224, 0.225]\n",
        "      )\n",
        "  ])\n",
        "\n",
        "  test_transform = transforms.Compose([\n",
        "      transforms.Resize((260, 260)),\n",
        "      transforms.Grayscale(num_output_channels=3),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize(\n",
        "          mean=[0.485, 0.456, 0.406],\n",
        "          std=[0.229, 0.224, 0.225]\n",
        "      )\n",
        "  ])\n",
        "\n",
        "  train_dataloader, test_dataloader, class_names , train_dataset, test_dataset, train_index, test_index = data_setup.create_dataloaders(\n",
        "    data_dir=data_dir,\n",
        "    train_transform=train_transform,\n",
        "    test_transform=test_transform,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    test_size=0.2  # 80/20 split\n",
        "  )\n",
        "  #=============================================================\n",
        "  # Engine\n",
        "  torch.manual_seed(42)\n",
        "  torch.cuda.manual_seed(42)\n",
        "  #=======================EfficientNet B2============================#\n",
        "  #=========================={PHASE 1}===============================#\n",
        "  # PHASE (1): Backbone\n",
        "  # EPOCH - 5\n",
        "  # LR - 0.001\n",
        "  # Backbone\n",
        "  # New optimizer [Mandatory]\n",
        "  # Weight decay [ADDed]\n",
        "  #\n",
        "  #=============================================================\n",
        "  weigths = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
        "  model = torchvision.models.efficientnet_b2(weights=weigths).to(device)\n",
        "  #=============================================================\n",
        "  num_classes = len(class_names)\n",
        "  model.classifier = torch.nn.Sequential(\n",
        "      torch.nn.Dropout(p=0.2, inplace=True),\n",
        "      torch.nn.Linear(in_features=1408, out_features=num_classes, bias=True)\n",
        "  ).to(device)\n",
        "\n",
        "  for param in model.features.parameters():\n",
        "      param.requires_grad = False\n",
        "  #=============================================================\n",
        "  #===============Loss function and Optimizer===================\n",
        "  loss_fun = torch.nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(model.classifier.parameters(), lr=0.001)\n",
        "  #=============================================================\n",
        "  print(\"======== PHASE 1: Training ==========\")\n",
        "  start_time = timer()\n",
        "  engine.initiate_train_test(\n",
        "    model=model,\n",
        "    train_dataloader=train_dataloader,\n",
        "    test_dataloader=test_dataloader,\n",
        "    loss_fun=loss_fun,\n",
        "    optimizer=optimizer,\n",
        "    epochs=5,\n",
        "    device=device\n",
        "  )\n",
        "  end_time = timer()\n",
        "  print(f\"[INFO] Total training time: {end_time-start_time:.3f} seconds\")\n",
        "  print(\"=======================================\")\n",
        "  #=================Utils for saving models=====================\n",
        "  utils.save_model(\n",
        "      model=model,\n",
        "      target_dir='models',\n",
        "      model_name=f'Recycle_Model_.pth'\n",
        "  )\n",
        "  #=============================================================\n"
      ],
      "metadata": {
        "id": "4HJAK01DEuGP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 344,
          "referenced_widgets": [
            "54e47d3d665b4e7981e402610469677a",
            "def502b1f5f14234bb700bbd34e5d626",
            "5b0b8d55b8404e52bf37ea1af53fa01b",
            "44cc07fe89b64a0baa0c4ad3a7d1e9c5",
            "f0d0a998336543e2acb3f5461130d6bd",
            "ab5553f68c504fb882eb009583454fff",
            "ee0afa93d8724902b25310aede1106b2",
            "8628dbff07064f61929cc2ab7b70d42c",
            "1c41ffea3b704f50b48fc63e8a3ac813",
            "d0548f8f58c0452fbf453061e865cdad",
            "3583f3c3a1b046639741cfc72166e29b"
          ]
        },
        "outputId": "43bf6539-86c5-4907-f14b-1e276c8763c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Info] Downloading feyzazkefe/trashnet ...\n",
            "[Info] Extracting dataset...\n",
            "[Success] Dataset ready at: data/dataset-resized\n",
            "[INFO] Total images: 2527\n",
            "[INFO] Training images: 2021\n",
            "[INFO] Testing images: 506\n",
            "Downloading: \"https://download.pytorch.org/models/efficientnet_b2_rwightman-c35c1473.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b2_rwightman-c35c1473.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 35.2M/35.2M [00:00<00:00, 180MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======== PHASE 1: Training ==========\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "54e47d3d665b4e7981e402610469677a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 | train_loss: 1.3056 | train_acc: 0.5538 | test_loss: 0.9732 | test_acc: 0.7452\n",
            "Epoch: 2 | train_loss: 0.9107 | train_acc: 0.7187 | test_loss: 0.7923 | test_acc: 0.7803\n",
            "Epoch: 3 | train_loss: 0.7598 | train_acc: 0.7508 | test_loss: 0.6939 | test_acc: 0.8155\n",
            "Epoch: 4 | train_loss: 0.6934 | train_acc: 0.7773 | test_loss: 0.6530 | test_acc: 0.8116\n",
            "Epoch: 5 | train_loss: 0.6563 | train_acc: 0.7804 | test_loss: 0.6304 | test_acc: 0.7999\n",
            "[INFO] Total training time: 76.332 seconds\n",
            "=======================================\n",
            "[INFO] Saving model to: models/Recycle_Model_.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [4] Random 3 IMG Testing"
      ],
      "metadata": {
        "id": "2X4ilSHBm-yX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from typing import List, Tuple\n",
        "import torch\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image # Import Image for PIL operations\n",
        "from pathlib import Path # Import Path to handle paths\n",
        "\n",
        "def pred_and_plot_img(model: torch.nn.Module,\n",
        "                     image_path: str,\n",
        "                     class_names: List[str],\n",
        "                     image_size: Tuple[int, int] = (224, 224),\n",
        "                     transform: torchvision.transforms = None,\n",
        "                     device: torch.device = \"cpu\"):\n",
        "\n",
        "    # 1. Load and prepare image\n",
        "    img_pil = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "    if transform is not None:\n",
        "        img_transform = transform\n",
        "    else:\n",
        "        img_transform = torchvision.transforms.Compose([\n",
        "            torchvision.transforms.Resize(image_size),\n",
        "            torchvision.transforms.ToTensor(),\n",
        "            torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                             std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "    # 2. Prediction logic\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "        # Move transformed image to device\n",
        "        transformed_img = img_transform(img_pil).unsqueeze(dim=0).to(device)\n",
        "        logits = model(transformed_img)\n",
        "\n",
        "        probs = torch.softmax(logits, dim=1)\n",
        "        pred_label_idx = torch.argmax(probs, dim=1).item()\n",
        "        pred_class = class_names[pred_label_idx]\n",
        "\n",
        "    # 3. Extract actual label safely\n",
        "    # This assumes your directory is: data_dir/class_name/image.jpg\n",
        "    actual_label = Path(image_path).parent.name\n",
        "\n",
        "    # 4. Plotting\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    plt.imshow(img_pil)\n",
        "\n",
        "    # Color code the title: Green for correct, Red for wrong\n",
        "    title_color = \"g\" if pred_class == actual_label else \"r\"\n",
        "\n",
        "    plt.title(f\"Actual: {actual_label} | Pred: {pred_class}\\nProb: {probs.max():.3f}\",\n",
        "              color=title_color)\n",
        "    plt.axis(False)"
      ],
      "metadata": {
        "id": "TUNiqfWXcY8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "# Instead of globbing the whole folder, use the indices from your split\n",
        "# Assuming 'test_dataset' is what was returned by your create_dataloaders function\n",
        "num_images_to_plot = 15\n",
        "for _ in range(num_images_to_plot):\n",
        "    # Get a random index from the test_dataset\n",
        "    random_idx = random.randint(0, len(test_index)-1)\n",
        "\n",
        "    # Get the image path from the underlying ImageFolder samples list\n",
        "    # test_dataset.indices maps the subset index back to the original index\n",
        "    original_idx = test_dataset.indices[random_idx]\n",
        "    image_path, _ = test_dataset.dataset.samples[original_idx]\n",
        "\n",
        "    pred_and_plot_img(\n",
        "        model=model,\n",
        "        image_path=image_path,\n",
        "        class_names=class_names,\n",
        "        transform=test_transform,\n",
        "        device=device\n",
        "    )"
      ],
      "metadata": {
        "id": "Iu0jLtwdTsa8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [5] Custom IMG TESTing"
      ],
      "metadata": {
        "id": "OF7u0t7KnE32"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from pathlib import Path\n",
        "import random\n",
        "\n",
        "def custom_img_testing(link: str):\n",
        "  custom_img_path = Path(f'data/custom_img_{round(random.random(), 3)}.jpg')\n",
        "\n",
        "  if not custom_img_path.is_file():\n",
        "    print(f\"Downloading custom image ... \")\n",
        "    with open(custom_img_path, \"wb\") as f:\n",
        "      request = requests.get(link)\n",
        "      f.write(request.content)\n",
        "  else:\n",
        "    print(f\"{custom_img_path} already exists\")\n",
        "\n",
        "  return custom_img_path\n",
        "\n",
        "\n",
        "pred_and_plot_img(\n",
        "    model=model,\n",
        "    image_path=custom_img_testing(link=\"https://previews.123rf.com/images/picsfive/picsfive2001/picsfive200100370/138798094-close-up-of-a-paper-ball-trash-on-white-background.jpg\"),\n",
        "    class_names=class_names,\n",
        "    transform=test_transform,\n",
        "    device=device\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "WEtLh-mKcbsd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}